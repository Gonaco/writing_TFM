

\chapter{Mapping on superconducting quantum processors}
\label{sec:org0c87802}

In this chapter we will describe the superconducting quantum processor used in this thesis and its constraints. 
Although the work done in this thesis  could be apply to any quantum technology, we limit our study to the surface code 17 chip (Surface-17) developed by Leo Di Carlo's group at QuTech \cite{Versluis_2017}.
In addition, we will explain the mapping model and the new metrics proposed in this work.

\section{Constraints of the Surface-17 chip}
\label{sec:org168a66d}
\subfile{chapters/constraints}

\section{Mapping model}
\label{sec:org19dc500}
In order to map quantum circuits on the superconducting quantum chip we will use the mapping model developed in our group \cite{Lao_2018}.
A mapping algorithm or mapper is an algorithm able to find a mapping solution given a quantum circuit and a target device.
We consider that a mapper is subdivided in three tasks: scheduler, initial placement and routing, as described in the \hyperref[sec:orgd680d43]{Mapping of quantum circuits} section.
Our procedure is modular, although the flow is restricted to the one described below.
The mapper is fully adaptable to any device constrain.
As we will explain, it also offers two different kinds of schedulers and three router possibilities, as well as the option of running or not the initial placement and another options related to the chip constrains.
We believe this solution will aid researchers to investigate the best mapping, with different configurations.
The mapping model works as follows.

\subsection{Initial placement}
\label{sec:org4fe48f6}

The first step is to map the virtual qubits (the ones of the circuit) to the physical ones (the ones in the quantum chip).
Once the mapper knows the qubits required by the algorithm, it will try to find the best initial placement for the given circuit, that is, the placement that requires less movements of qubits.
First, it will check the first set of two-qubit gates and their target qubits.
If those qubits are not NN, it will initiate an Integer Liner Programming (ILP) algorithm \cite{Lao_2018} to find the optimal initial placement.
This optimal initial placement tries to place the qubits in a way that the minimum number of qubit movements are required.


\subsection{Router}
\label{sec:org18105df}
The function of the router is to move non-neighbouring qubits to adjacent positions whenever they need to interact. To this purpose, our router inspects all the gates, one by one, looking for non-NN qubits interactions.
Whenever the router encounters one, it will calculate several shortest paths -- the ones that use the minimal amount of SWAPs -- and selects one of them. This path is selected based on the circuit depth, that is, the path that better interleaves with the previous operations.
Then, it will insert the SWAP operations (decomposed or not decomposed) required to follow the selected path.



There are three router options, and a different path is selected depending on the router option that was selected by the user:

\begin{enumerate}
\item \texttt{base} finds all the shortest paths and selects one of them randomly. The shortest paths are calculated based on the Manhattan distance, counting the number of steps that the qubits will need to move.
\item \texttt{minextend} finds all the shortest paths as well, but selects the one that will minimally increase the circuit depth. In this case, only gate dependencies are considered, not chip constraints that limits the parallelism.
\item The \texttt{minextendrc} approach is exactly the same as the second one, but taking into account the chip constraints when selecting the path.
\end{enumerate}


\subsection{Scheduler}
\label{sec:org2ec75f3}

The RC-scheduler will finally schedule the circuit using either an ALAP or an ASAP approach, depending on the user requirements.
It will take into account the chip parallelism constraints, as the ones described in \ref{sec:org168a66d}.

One downside factor regarding this methodology is that the mapper only takes into account one gate every time.
It does not look ahead in order to decide the best path for next iterations.
Or, what is the same, it does not always find the best mapping solution.
This limitation comes from the fact that, the more steps you are looking ahead while mapping, the more computationally complex will the mapper be.
We highlight that the mapping problem is an NP problem \cite{Siraichi_2018}.
Therefore, this method represents a viable solution, although a lot of future work should be done.

\section{New Mapping Metrics: algorithm reliability}
\label{sec:org27990de}
As mentioned in the \hyperref[sec:orgc729465]{Mapping metrics} section in the second chapter, we name \emph{mapping metrics} to those metrics used to assert the quality of a mapper and that are also used by the mapper algorithm as a cost function to optimize.
In this section, we will present and  define some new mapping metrics: \textbf{fidelity}, \textbf{probability of success} and \textbf{Quantum Volume}.

\subsection{Fidelity and Probability of success}
\label{sec:org0c7b2c2}

\begin{figure}
    \centering

\resizebox{0.3\textwidth}{!}{
   \Qcircuit @C=1em @R=.7em {
&&&&&\mbox{fidelity}&&&&\mbox{prob. success}\\
&&&&&&&&&\\
\lstick{a} & \targ & \qw & \qw & \qw & \qw \ar@{.}[]+<0.75em,1em>;[d]+<0.75em,-9em> & \meter \ar@{.}[]+<2em,1em>;[d]+<2em,-9em> & \rstick{0} \qw&&\\
\lstick{b} & \ctrl{-1} & \targ & \qw & \qw & \qw & \meter & \rstick{0} \qw&&\\
\lstick{c} & \qw & \ctrl{-1} & \targ & \qw & \qw & \meter & \rstick{0} \qw&&\\
\lstick{d} & \qw & \qw & \ctrl{-1} & \targ & \qw & \meter & \rstick{0} \qw&&\\
\lstick{e} & \qw & \qw & \qw & \ctrl{-1} & \targ & \meter & \rstick{0} \qw&&\\
\lstick{f} & \qw & \qw & \qw & \qw & \ctrl{-1} & \meter & \rstick{1} \qw&&
}
}
\caption{Example of the fidelity and probability of success calculation in the Gray encoder quantum circuit}
\label{fig:latency_swaps_ex_orig}
\end{figure}

Fidelity and probability of success  are two different ways to measure how errors affect the algorithm reliability.
Fidelity is measured before measuring the qubits, and it is the difference between the obtained quantum state (affected by errors) and the expected quantum state (the one without errors).  Probability of success is obtained after measuring the qubits, and then  after collapsing the quantum state. Note that in this case measurement errors are also considered. After running the algorithm several times, it shows how many times a correct result is measured.

Although quantum fidelity is not taking into account measurements, the quantum fidelity metric is quite related to the probability of success.
Both compare two quantum states, but, while the success of an algorithm is a boolean metric -- either success or failure --, the fidelity returns a number of how different the states are; how far is one state from the other.
As an analogy, if we consider the quantum states as vectors, the fidelity would be the dot product between them.
However, the fidelity between orthogonal states -- states that differ in rotations around the x-axis as \(| 0100 \rangle\), \(| 0001 \rangle\) and \(| 1111 \rangle\) -- is also 0.
In eq. \ref{eq:orge3c164e} the fidelity formula is shown to understand its behaviour.

\begin{equation}
\label{eq:orge3c164e}
{\displaystyle F(\rho ,\sigma )=\left(\operatorname {Tr} {\sqrt {{\sqrt {\rho }}\sigma {\sqrt {\rho }}}}\right)^{2},}
\end{equation}

where \(\rho =\sum _{i}p_{i}|i\rangle \langle i|\) and \(\sigma =\sum _{i}q_{i}|i\rangle \langle i|\).
These states are named \textbf{mixed states} and they are defined as the weighted sum of different pure states.
This is the general case in order to calculate the fidelity in any case.
But, for a situation with one of the states as pure state (\({\displaystyle \rho =|\psi _{\rho }\rangle \!\langle \psi _{\rho }|}\)) the calculation of fidelity reduces its complexity as it can be understood from eq. \ref{eq:org2703857}.
Eq. \ref{eq:org1f331e2} shows the next step of simplicity, the one in which both states are pure (\({\displaystyle \rho =|\psi _{\rho }\rangle \!\langle \psi _{\rho }|}, {\displaystyle \sigma =|\psi _{\sigma }\rangle \!\langle \psi _{\sigma }|} {\displaystyle \sigma =|\psi _{\sigma }\rangle \!\langle \psi _{\sigma }|}\)).


\begin{equation}
\label{eq:org2703857}
{\displaystyle F(\rho ,\sigma )=\operatorname {Tr} \left[{\sqrt {|\psi _{\rho }\rangle \langle \psi _{\rho }|\sigma |\psi _{\rho }\rangle \langle \psi _{\rho }|}}\right]^{2}=\langle \psi _{\rho }|\sigma |\psi _{\rho }\rangle \operatorname {Tr} \left[{\sqrt {|\psi _{\rho }\rangle \langle \psi _{\rho }|}}\right]^{2}=\langle \psi _{\rho }|\sigma |\psi _{\rho }\rangle .}
\end{equation}

\begin{equation}
\label{eq:org1f331e2}
{\displaystyle F(\rho ,\sigma )=|\langle \psi _{\rho }|\psi _{\sigma }\rangle |^{2}}
\end{equation}


We define probability of success as the probability of having the correct or expected results after running and measuring several times a quantum algorithm.
For instance, if the expected result of an algorithm is \texttt{0100} and the results are either \texttt{0001} or \texttt{1111} we will get a failure.
We will only get a success if the algorithm returns \texttt{0100}.
Then, in the previous example, if we get \texttt{0001}, \texttt{0100} and \texttt{1111} as results after running the algorithm three times the resulting probability of success will be \(\frac{1}{3}\).

\textcolor{red}{Add intuition to definition of fidelity}

\textcolor{red}{Make definition of prob of success stand-out and/or give it a private subsection so that glancing readers see it easier}

\textcolor{red}{Change this. Both metric need several runs. The fidelity can only be calculated by simulation but not in real experiments. Mention again that we will use deterministic algorithms and the the result (after measuring) is only affected by errors and not by the non-deterministic behaviour of most of the quantum algorithms}
This model was chosen because it is one of the most practical ways to include the measurement gate into the error analysis.
But, at the same time, we aware that this metric have a limitation: it cannot assert -- in just one run -- the success of an algorithm that ideally returns a quantum state different that either the ground or the excited state.
Once a quantum state is measured it collides in either ground or excited.
In order to extract the complete quantum state, several runs of the circuit should be done, inheriting the results as a probabilistic distribution or histogram.
For a state as \(0.3 | 0 \rangle + 0.7 | 1 \rangle\), after several measurements, we will get \texttt{0} a 30\% of the times and \texttt{1} a 70\% of the times.
Accordingly, in order to calculate the probability of success, we will need to compare histograms instead of only one measured result.



In our study, we will use both metrics in order to study how the mapping process and then the amount of errors affect the  algorithm reliability.
Note that as we already mention, the fidelity is calculated before measurement and then it does  not take into account the measurement errors. It could be the case that a quantum state is erroneous before the measurement and, then, due to the error added from the measurement the erroneous state would flip and converge into the expected state.
At the same time, a correct quantum state could be measured wrongly due to the measurement errors.

\subsection{Quantum Volume}
\label{sec:org3029336}

Another metric that could be useful for the mapping quality assessment is Quantum Volume \cite{Bishop_2017,Moll_2018}.
Quantum Volume is a metric proposed by IBM to depict whether a device is able to run a quantum circuit or not.
Given the different hardware implementations and technologies in Quantum Computation (superconducting, ion-trap, spin qubits, \ldots{}), it is often difficult to benchmark the usefulness or power of quantum systems. 
The aim of Quantum Volume is to quantify the computational power of quantum devices. 
Consequently we will use it as a metric to measure the runnability of the quantum algorithms on the quantum devices.
But, while the device is the target of the Quantum Volume metric, we focus on the circuit.
Our goal is to assess how the mapping procedure affects the runnability of a given circuit and to study how the Quantum Volume is related to the fidelity and the probability of success.

The general Quantum Volume formula is defined in eq. \ref{eq:general_qv} where \(N\) is the number of physical qubits and \(d(N)\) is the achievable circuit depth, i.e. the maximum circuit depth for which the results, after running it on some device, are correctable and useful.
\(d(N)\) depends on the number of qubits in the circuit as well as on the error rate of the quantum device.

\begin{equation}
\label{eq:general_qv}
V_Q = \min (N, d(N))^2
\end{equation}

In our case, we want to relate the Quantum Volume of a device with the circuit.
Therefore, we define the \textbf{algorithms Quantum Volume} and the \textbf{runnability} of the quantum circuit on a given device.

As with \(V_Q\), we initially derived the algorithm's Quantum Volume from the general equation \(V_Q\) (see eq. \ref{eq:orgc799c10}), although we will adapt it later.

\begin{equation}
\label{eq:orgc799c10}
V_Q^a = \min \left[ n,d \right]^2
\end{equation}

Note that \(d\) is not \(d(N)\) but the real depth of the given algorithm.
At the same time, \(n\) is the number of qubits required by the algorithm itself.

We are aware that this approach has a limitation regarding the mapping of the quantum circuit.
As explained before, \(V_Q\) is able to take into account the sophistication of the mapping procedure.
It is inherited in the \emph{model algorithm}.
But, in this case, the \(V^a_Q\) of an algorithm before and after mapping will remain the same.
After mapping an algorithm, the usual effect is an increase in the depth and the number of operations.
Rare mapping methods consider the qubit addition in the technique.
And, even considering it, \(n\) is not often growing too much in comparison with \(d\).
In the current NISQ era, the quantum circuits need much less qubits than depth.
Therefore, most of the times, the minimum value between \(n\) and \(d\) will be \(n\).
As soon as \(V^a_Q\) is taking into account the minimum of them and the mapping procedure affects mostly to \(d\) we can conclude that this definition of \(V^a_Q\) is not considering the mapping in its results.

A simplified solution for this problem would be the \(V^a_Q\) definition as the multiplication between \(n\) and \(d\) (see eq. \ref{eq:org6aba4aa}).
Unfortunately, this approach has several drawbacks as well.
As Moll et al. point out \cite{Moll_2018}, extreme cases of high \(n\) and low \(d\) -- or the other way around -- lead to inconsistencies of the multiplication metric.
But, considering that most of our work is not going to be in any of these extreme cases and that we can avoid those outliers, we define the algorithm's Quantum Volume as:

\begin{equation}
\label{eq:org6aba4aa}
V_Q^a =  n \times d
\end{equation}

Finally, once the Quantum Volume of an algorithm is stated, we define runnability as the condition for which the \(V_Q\) should be bigger than \(V^a_Q\).
That is the condition that the computational power of the device should be bigger than the computational power required by the algorithm.

$$\text{Runnable if: } V_Q > V^a_Q \quad \quad \text{ when } N \ge n$$

\textcolor{red}{also give this formula a label (3.7)}

For instance, in order to understand this concept, one may imagine the process of checking, whether or not, some cube with a given volume -- representing the algorithm -- would fit in a box -- the device --.
If the algorithm's box volume is smaller than the volume of the device's box, the algorithm's box will fit inside.

Indeed, one acceptable criticism of this definition is that, as \(V_Q\) and \(V^a_Q\) are finally defined in the previous sections, it seems that it is not really fair to compare them.
But, as soon as the general behaviour of both definitions of \(V^a_Q\) is the same we believe that this definition of runnability is mathematically correct and useful.
