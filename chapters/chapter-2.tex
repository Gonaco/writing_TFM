
\chapter*{Quantum computers and mapping of quantum circuits}
\label{sec:org78bbf1d}
\section*{State of the art}
\label{sec:org93b06c4}

Quantum algorithms are meant to leverage the promising power of quantum computers \cite{coles18:quant_algor_implem_begin}.
Commonly described as quantum circuits, quantum algorithms are hardware agnostic.
Due to the variety of technologies that emerged with diverse specifications, there is a vast amount of literature on the algorithms -- high abstraction level --, neglecting hardware constraints.
From ion traps [? paper on ion traps] to superconducting qubits \cite{Barends_2014,Versluis_2017}, through quantum dots \cite{Hill_2015,Li_2018}, each layout has its own requirements and constraints.
Although all of them are arranged in a 2D -- planar -- structure, ion traps technologies are capable to connect the qubits in all-to-all networks while superconducting technologies connections form a grid shaped network where each qubit is connected to a maximum of four neighbors.
This grid behaviour establishes one of the main limitation in today's quantum chips, the NN constraint.
Also industry's superconducting chip layouts from IBM \cite{IBM_QX}, Google \cite{boixo16:charac_quant_suprem_near_term_devic} and Rigetti \cite{Sete_2016} follow this structural limitations.

As for the high abstraction level of the quantum algorithm, a link between the algorithms and the devices is required \cite{Fu_2016}.
As in classical computation, the algorithms should go through a compilation process in order to adapt them to the hosting device.
Certainly, the mapping procedure is an important part of this process based on three sub-tasks: scheduling, initial placement and routing; as we considered before.

There is a considerable amount of literature on the mapping task.
Initial works on this field \cite{Metodi_2006,Whitney_2007,Bahreini_2015} focused primarily on the definition of what they characterized as a \emph{scheduler} able to parallelize operations and add the required gates to route qubits.
They would consider general constraints, common for most of the hardware devices -- although the works were examining ion-traps as hardware implementations.
The proposed techniques examine a dependency graph looking for the best way to organize qubits and operations.
The majority of the methods use latency as the metric to minimize, however some of them \cite{Farghadan_2017} would minimize in number of SWAP operations.
Following a similar reasoning as the first approaches, more complex solutions \cite{booth18:compar_integ_const_progr_tempor} have been published.
Also, several publications \cite{Lye_2015,Wille_2016} outlining only the routing sub-task using the number of SWAPS as the metric to minimize.
A recent review of the literature on the mapping topic \cite{zulehner17:effic_method_mappin_quant_circuit,Siraichi_2018,mckay18:qiskit_backen_specif_openq_openp_exper,Dueck_2018,Venturelli_2018} focused on device specific mapping algorithms, with promising results.

In addition to the previous limitations, quantum devices -- no matter which technology -- are error prone.
Quantum operations are faulty and qubits are not able to hold the desired state for long times, gradually rotating to another state -- the qubit decoheres.
\uline{[some numbers for the technologies]} \cite{O_Brien_2017}.
This creates an undesirable environment to compute the most useful algorithms.
Therefore, in order to fight the errors generated by this behaviour, fault-tolerant (FT) and quantum error correction (QEC) mechanisms have been developed during the last years \cite{Nielsen_2009} [? papers on error correction].
These techniques force the quantum chips layout to arrange the qubits in a particular manner, constraining them even more.

Many attempts have been made \cite{Dousti_2014,Heckey_2015,hwang18:hierar_system_mappin_large_scale,murphy18:contr,Lao_2018} with the purpose of develop a FT mapping able to work at the logical -- qubit -- level.
However, due to the high complexity of the QEC techniques, quantum chips with large amounts of qubits are still theory.
More recent evidence \cite{Preskill_2018}, proposes the Noisy Intermediate-Scale Quantum (NISQ) devices as the next step for near future hardware with an amount of 50-100 qubits and without QEC or much simpler encodings.
Several studies, for instance \cite{tannu18:case_variab_aware_polic_nisq,paler18:nisq,paler18:influen_initial_qubit_placem_durin}, have been conducted on the mapping algorithms required for NISQ devices.

\section*{Mapping explaining the metrics}
\label{sec:org68432cc}

As outlined in the state of the art, the literature considers mainly two metrics to report the quality of their mapping algorithms, the added \textbf{latency} and the \textbf{number} of introduced \textbf{operations} after the mapping procedure.
Latency is the time required to run a quantum circuit in a given quantum device.
It depends on the chip cycle time and the depth of the circuit, that is the number of cycles in the algorithm.
On the other hand, the number of operations is ..

Both stand on correct -- but not precise -- reasoning.
The latency metric states that the longer the circuit, the more errors would appear.
Indeed, time is a main issue in quantum computing.
Decoherence time is not only a maximum qubit lifetime, the quantum state would be harder to hold for times closer to the decoherence time.
So, 

The more these metrics grow, the worst the mapping is.
Research has tended to focus on either latency or number of SWAPS rather than the error produced by the mapping.
