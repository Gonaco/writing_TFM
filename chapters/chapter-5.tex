
\chapter{Results}
\label{sec:orgaed8c93}

In this chapter the simulation results are presented with reference to the aim of the thesis, which was to study the mapping metrics:
their precision when assessing the mapping quality and their relation with the error amount after mapping.
First, in the \hyperref[sec:org2563966]{Impact of the mapping on the algorithm reliability} section we show how the mapping procedure affects the general increment of errors.
After that, we analyze the metrics and their correlation with the error increase in the \hyperref[sec:orgb1f6186]{Analysis of the mapping metrics} section.

\section{Impact of the mapping on the algorithm reliability}
\label{sec:org2563966}
After the selection of benchmarks in the \href{chapter-4.org}{Benchmarks} section, we started their simulations using our simulation framework.
Some of the benchmarks either had very long simulation times or were even impossible to simulate due the computational power required by the quantumsim's complex error model. 
In most of those cases the circuits had more than ten qubits.
As we mentioned before throughout this thesis, the simulation of quantum systems is computationally exhausting.
The higher the number of qubits or the length of the circuit, the harder is it to simulate it.
Indeed, it is a critical issue in our case, as soon as we need to run multiple simulations in a complex error model.
Therefore, as can be seen in Tab. \ref{tab:map_selected_benchs}, we address that the final benchmark selection has a limitation in the number of qubits.

\begin{table}[htbp]
\caption{\label{tab:map_selected_benchs}
Table of the selected benchmarks to be mapped.}
\centering
\small
\begin{tabular}{lrrr}
\hline
Benchmark & \# qubits & \# gates & two-qubit gates (fraction)\\
\hline
\texttt{graycode6\_47} & 6 & 5 & 1.000\\
\texttt{xor5\_254} & 6 & 7 & 0.714\\
\texttt{4mod5\_v0\_20} & 5 & 20 & 0.500\\
\texttt{ham3\_102} & 3 & 20 & 0.550\\
\texttt{mod5d1\_63} & 5 & 22 & 0.591\\
\texttt{4gt11\_82} & 5 & 27 & 0.667\\
\texttt{rd32\_v0\_66} & 4 & 34 & 0.471\\
\texttt{alu\_v0\_27} & 5 & 36 & 0.472\\
\texttt{4mod5\_bdd\_287} & 7 & 70 & 0.443\\
\texttt{one\_two\_three\_v3\_101} & 5 & 70 & 0.457\\
\texttt{decod24\_bdd\_294} & 6 & 73 & 0.438\\
\texttt{alu\_bdd\_288} & 7 & 84 & 0.452\\
\texttt{one\_two\_three\_v1\_99} & 5 & 132 & 0.447\\
\texttt{mod10\_176} & 5 & 178 & 0.438\\
\texttt{4gt12\_v1\_89} & 6 & 228 & 0.439\\
\texttt{hwb4\_49} & 5 & 233 & 0.459\\
\texttt{4gt4\_v0\_72} & 6 & 258 & 0.438\\
\texttt{decod24\_enable\_126} & 6 & 338 & 0.441\\
\texttt{mod8\_10\_177} & 6 & 440 & 0.445\\
\texttt{mod5adder\_127} & 6 & 555 & 0.431\\
\texttt{sf\_276} & 6 & 778 & 0.432\\
\texttt{sf\_274} & 6 & 781 & 0.430\\
\texttt{sym6\_145} & 7 & 3888 & 0.438\\
\hline
\end{tabular}
\end{table}


As explained in the \href{chapter-4_old.org}{Analysis framework} section, after running one benchmark for one thousand times, the results obtained are the fidelity and the probability of success.
We also extracted other metrics like the number of SWAPs added, the depth of the circuits and the Quantum Volume, among other circuit statistics.
These metrics were obtained for all mentioned benchmarks before (non-mapped) and after mapping (mapped). 
Note that, for a fair comparison both the mapped and the non-mapped circuits have been decomposed into the gates supported by  the Surface-17 chip (see Fig. \ref{fig:decompositions}). 
For the mapped ones, we use the three router algorithms developed in our group (see \hyperref[sec:org19dc500]{Mapping Model}): \textit{base}, \textit{minextend} and \textit{minextendrc}.

In quantumsim, we also try different configurations regarding the decoherence time in order to study the mappers in different error regimes. 
All the results are detailed in \href{appendix-1.org}{Appendix A}.


In order to illustrate how the mapping process affects the algorithm increment of errors, Figure \ref{fig:f_diff_bar_plot} shows the fidelity for some of the benchmarks before being mapped and after being mapped using the \texttt{minextendrc} router.
We can see that fidelity is smaller for long circuits -- like \texttt{sf\_274} or \texttt{mod5adder\_127} -- than for the short ones -- \texttt{graycode6\_47} or \texttt{xor5\_254}.
It can be seen that, for the long circuits, the fidelity can even decrease more than 50\%.
For instance, \texttt{sf\_274}'s fidelity goes from 0.35 to 0.17, \texttt{mod5adder\_127}'s goes from 0.45 to 0.19.
On the other hand, for the small circuits like \texttt{graycode6\_47} or \texttt{xor5\_254} the fidelity decreases around 1\%; from 0.99 to 0.98 and from 0.99 to 0.97, respectively.
For more details, we show the exact result values for all the benchmarks in \href{appendix-1.org}{Appendix A}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/f_diff_bar_plot.eps}
\caption{\label{fig:f_diff_bar_plot}
Difference of fidelities before and after mapping with the \emph{minextendrc} router for five different benchmarks.}
\end{figure}
As we mentioned before, we use three different routers, each one giving a different mapped version per benchmark and, therefore, different metric statistics.
For instance, as it can be seen in \href{appendix-1.org}{Appendix A} and in Tab. \ref{tab:depth_per_bench}, the depth of the \texttt{graycode6\_47} circuit before being mapped is 32 cycles and after being mapped grows to 111, 61 or 82 depending on the router.
In Fig. \ref{fig:mapping_effect_3000_diff_lines} and Fig. \ref{fig:mapping_effect_1000_diff_lines} we plot the fidelity of the benchmarks mapped and non-mapped, where the dots are the different benchmark configurations aligned vertically and ordered from the shortest to the longest circuit depth; from left to right.
The dark blue dots represent the benchmarks before being mapped and the light blue ones represent the different mapped version of it from the different routers.
Fig. \ref{fig:mapping_effect_3000_diff_lines} presents the result with a decoherence time of \(30 \mu s\) and Fig. \ref{fig:mapping_effect_1000_diff_lines} the ones with a decoherence time of \(10 \mu s\).
As mentioned in the \href{quantum_computing.org}{Qubits are faulty} section, the shorter the decoherence time the more errors would appear.
From these results, we observe that, in general, the fidelity starts from lower points just as the size of the circuit increases.
This behaviour is certainly because the longer the circuit, the more errors it will get.

In Fig. \ref{fig:mapping_effect_diff_3000} and Fig. \ref{fig:mapping_effect_diff_1000} we plot the maximum (red) and minimum (blue) percentage of difference in fidelity between the non-mapped version of the benchmarks and each one of the mapped versions.
We calculate the percentage of the fidelity difference as \(\frac{f_{\text{before}} - f_{\text{after}}}{f_{\text{before}}}\) where \(f_{\text{after}}\) is the fidelity of the mapped circuit and f\(_{\text{before}}\) is the fidelity of the non-mapped one.
As an example, in Fig. \ref{fig:mapping_effect_3000_diff_lines} and Fig. \ref{fig:mapping_effect_1000_diff_lines}, we plot two lines over some benchmark to depict the maximum (red) and the minimum(blue) difference.

We can observe how the fidelity difference between non-mapped and the mapped algorithms tends to grow.
As soon as most of the selected benchmarks have a similar two-qubit gates percentage and also a similar number of qubits -- main parameters for the mapping task --, we can say that this increasing tendency is mostly provoked by the length of the circuit before being mapped.
Moreover, another remark we see is that, in the case of a decoherence time of \(10 \mu s\) (Fig \ref{fig:mapping_effect_1000}), we observe strange results like negative fidelities. 
This is due to the chaotic behaviour of the quantum noise; the more it affects the system, the more chaotic results appear.
As we explained in the \href{quantum_computing.org}{Qubits are faulty} section, the decoherence time is related with the amount of errors that affect our circuit; the shorter the decoherence time, the more errors will appear in our quantum system.

Finally, in Tab. \ref{tab:depth_per_bench} we show the exact depth values of each one of the benchmarks in the order they appear, from left to right.
For instance, the first column of dots to the left is the \texttt{graycode6\_47} circuit.



\begin{figure}
\centering
\subfigure[Fidelity per benchmark]{

\includegraphics[width=0.7\textwidth]{figures/mapping_effect_3000_diff_lines.eps}

\label{fig:mapping_effect_3000_diff_lines}
}

\subfigure[Difference of fidelity per benchmark]{

\includegraphics[width=0.7\textwidth]{figures/mapping_effect_diff_3000.eps}

\label{fig:mapping_effect_diff_3000}
}

\caption{Impact of mapping for $t_d = 30 \mu s$}
\label{fig:mapping_effect_3000}
\end{figure}


\begin{figure}
\centering
\subfigure[Fidelity per benchmark]{

\includegraphics[width=0.7\textwidth]{figures/mapping_effect_1000_diff_lines.eps}

\label{fig:mapping_effect_1000_diff_lines}
}

\subfigure[Difference of fidelity per benchmark]{

\includegraphics[width=0.7\textwidth]{figures/mapping_effect_diff_1000.eps}

\label{fig:mapping_effect_diff_1000}
}

\caption{Impact of mapping for $t_d = 10 \mu s$}
\label{fig:mapping_effect_1000}
\end{figure}


\begin{table}[htbp]
\caption{\label{tab:depth_per_bench}
Different depth per benchmark}
\centering
\tiny
\begin{tabular}{lrrrr}
\hline
Benchmark & Depth before mapping & Depth after mapping with \emph{minextendrc} & Depth after mapping with \emph{minextend} & Depth after mapping with \emph{base}\\
\hline
\texttt{graycode6\_47} & 32 & 111 & 61 & 82\\
\texttt{mod5d1\_63} & 59 & 209 & 136 & 146\\
\texttt{ham3\_102} & 60 & 127 & 121 & 98\\
\texttt{alu\_v0\_27} & 80 & 248 & 156 & 214\\
\texttt{miller\_11} & 112 & 307 & 278 & 231\\
\texttt{one\_two\_three\_v3\_101} & 143 & 440 & 302 & 323\\
\texttt{decod24\_bdd\_294} & 144 & 407 & 328 & 300\\
\texttt{alu\_bdd\_288} & 165 & 495 & 383 & 360\\
\texttt{one\_two\_three\_v1\_99} & 256 & 839 & 530 & 609\\
\texttt{mod10\_176} & 327 & 1090 & 687 & 734\\
\texttt{hwb4\_49} & 439 & 1387 & 961 & 1006\\
\texttt{mini\_alu\_167} & 516 & 1598 & 992 & 1274\\
\texttt{decod24\_enable\_126} & 612 & 1788 & 1440 & 1446\\
\texttt{mod8\_10\_177} & 794 & 2275 & 1761 & 2006\\
\texttt{mod5adder\_127} & 944 & 2878 & 2667 & 2378\\
\hline
\end{tabular}
\end{table}

\section{Analysis of the mapping metrics}
\label{sec:orgb1f6186}
Once we acknowledge the behaviour of the fidelity decrease due to the mapping process, in this section, we will analyze which circuit parameter affects the amount of error the most.
We consider that the best mapping is the one adding the less possible errors to the original circuit.
If we are able to answer which metric is more correlated with the appearance of errors in the circuit, we will know the most critical metric for the mapping quality.
Therefore, in this section we evaluate the behaviour of the number of gates, the number of two-qubit gates -- in accordance with the number of SWAPs --, the depth and the Quantum Volume metrics in comparison with the error growth, or what is the same, the fidelity and the probability of success.


First we analyze the probability of success and fidelity correlation.
As we explained in the \hyperref[sec:org0c7b2c2]{Fidelity and Probability of success} section, one of the main differences between these two metrics is that the probability of success takes into account the measurement process of the qubits, while fidelity does not.
This makes the probability of success not only sensitive to the errors related with the measurement but also to the inherent non-deterministic behaviour of it.
This means that, although our benchmarks are deterministic, a circuit could result erroneously in a state in superposition due to the quantum noise, forcing the measurement to collapse to either the correct or the wrong state.
For example, let us say that we are expecting \texttt{1} as the result of a given qubit but, before measuring instead of \(|1\rangle\) we got \(\sqrt{0.3} |0\rangle + \sqrt{0.7} |1\rangle\).
In this case, without taking into account the measurement errors, the measurement will result in \texttt{0} with a 30\% chance and \texttt{1} with a 70\% chance.
For this reason, the probability of success tends to be more chaotic in regimes with high amount of errors; as we will throughout this section.



In Fig. \ref{fig:f_ps_correlation_with_meas_error} we plot the correlation of probability of success and fidelity. 
For this figure and the figures from now on in this section, each dot is a different benchmark configuration (see \href{appendix-1.org}{Appendix A}) and the colors represent different decoherence times, blue for 30 \(\mu s\) and orange for 10 \(\mu s\).


\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/f_ps_correlation.eps}
\caption{\label{fig:f_ps_correlation_with_meas_error}
Correlation between fidelity and probability of success for two different decoherence times}
\end{figure}



As expected, our experiments prove that both metrics are highly correlated.
We also appreciated the fact that most of the samples are above the \(x=y\) line; meaning that the probability of success, in general, is higher than the fidelity.
This could suggest that the measurement is 'correcting' circuit errors colliding the state in the correct result, instead of the wrong one.
It seems that the measurement is committing 'good' mistakes that result in the expected solution.
Nevertheless, this behaviour could be happening because of the indeterministic behaviour of the measurement or due to the fact that our algorithms are deterministic.
It would possible that this 'correcting- errors behaviour' is diluted while running quantum algorithms that result in a quantum state in superposition.
Further research should be done in order to understand this behaviour.

Another observation we see is that, the closer the samples get to 0 fidelity or 0 probability of success, the more chaotic the values are. Actually we can delimit two parts in the graph: above and below 0.7 in fidelity -- around 0.8 in probability of success. Above 0.7 we observe an almost linear behaviour and below 0.6 we start seeing more chaotic results. This behaviour happens because, whenever a circuit accumulates a high amount of errors, fidelity or probability of success low but almost random values.
Finally, we see that most of the orange samples are below the 0.6 fidelity value; or what is the same, most of the benchmarks simulated with the low decoherence time have more chaotic values. 
This can be explained by the fact that the correctness of circuits that are run in lower error rate regimes will have much better results in systems with higher error rates.


Regarding the rest of the metrics, below we analyze their relation with the fidelity and probability of success.
Apart from the metrics we described throughout this thesis, we did the analysis with the total number of gates as well, because we noticed a high correlation between that and the fidelity or the probability of success, although more with the first one.


The results of the main mapping metrics against fidelity are depicted in Fig. \ref{fig:f_metrics_correlation}.
We observe that, for all the cases, the fidelity decreases with a behaviour that looks inverse exponential and that decreases faster for small decoherence times.
Certainly, the shorter the decoherence times we use the more benchmarks will have non-useful results.
We can also see how fidelity never goes to zero, but it gets constant around 0.2, giving random results.
We consider the point where fidelity is constant as the limit in terms of each one of the variables.
We plot a line for the orange samples to mark this point.
Finally, it can be seen how the number of gates is the metric most related with the fidelity; it is the one with the samples more ordered.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/f_metrics_correlation_poly.eps}
\caption{\label{fig:f_metrics_correlation}
Correlation between fidelity and the mapping metrics.}
\end{figure}

The correlation between the probability of success and the other metrics can be see in Fig. \ref{fig:ps_metrics_correlation}.
We also observe a certain correlation between the metrics and the probability of success.
We observe a decreasing behaviour, although the samples are much more spread in this case than in the case of the fidelity. We believe that this is provoked by the indeterministic effect in the nature of the measurement.
We can see again that for shorter decoherence times the probability of success is lower and descends much faster. 


\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/ps_metrics_correlation.eps}
\caption{\label{fig:ps_metrics_correlation}
Correlation between probability of success and the mapping metrics.}
\end{figure}

In general, we observe a high correlation between all of the metrics and the fidelity and the probability of success; so in order to differ between the metrics, we calculate the Pearson correlation coefficients to measure the correlation quality between them for different amounts of errors in a circuit.
Note that the Pearson coefficient measures linear correlations and, as it looks like in Fig. \ref{fig:f_metrics_correlation}, the metrics behave in an inverse exponential fashion against fidelity.
For this reason, in order to see the real correlation between the metrics and the fidelity, we applied a \(log(\cdot)\) transformation to our fidelity data in order to make it linear for the Pearson calculation.


As it can be seen in the Pearson values (Tab. \ref{tab:pearson_corr_f} and Tab. \ref{tab:pearson_corr_ps}) the most correlated metric is the number of two-qubit gates.
These results hold the fact that the quality of the mapping depends directly on the length of the targeted circuit before it is being mapped.
For example, a long circuit well mapped will have always worse results in fidelity or probability of success than a short circuit badly mapped.
As Tab. \ref{tab:pearson_corr_f} and Tab. \ref{tab:pearson_corr_ps} highlight, we have a worse correlation for the shorter decoherence time.
This lack of correlation can be attributed to the fact that the majority of the samples with \(t_d = 1000\) are highly affected by the errors and, therefore, the samples have more random values.

Moreover, contrary to our expectations, the Quantum Volume was the least correlated metric.
This small lack of correlation can be attributed to the imprecise formula that we chose to compute it.
Future work needs to be done to devise a better formula.

Finally, if we compare both tables, we can see that the fidelity is more correlated with the metrics than the probability of success.
It is very likely that the reason for this result is that the measurement error together with the measurement indeterministic behaviour  adds more noise and spreads our samples.
Also the 'correcting-errors behaviour' of the measurement should be taken into account.

\begin{center}
\captionof{table}{\label{tab:pearson_corr_f}
Pearson correlation coefficient of the log transformation of fidelity against the metrics(\(\rho _{log(f),Y}\)), where \(Y\) is one of the four metrics we analyze}
\begin{tabular}{lrrrr}
\hline
 & \# of Gates & \# of Two-qubit gates & Depth & \(V_Q\)\\
\hline
\(t_d = 3000\) & -0.9730 & -0.9600 & -0.9455 & -0.9118\\
\(t_d = 1000\) & -0.8466 & -0.8135 & -0.8093 & -0.7736\\
\hline
\end{tabular}
\end{center}

\begin{center}
\captionof{table}{\label{tab:pearson_corr_ps}
Pearson correlation coefficient for the probability of success against the metrics (\(\rho _{p_s,Y}\)), where \(Y\) is one of the four metrics we analyze}
\begin{tabular}{lrrrr}
\hline
 & \# of Gates & \# of Two-qubit gates & Depth & \(V_Q\)\\
\hline
\(t_d = 3000\) & -0.9363 & -0.9248 & -0.9179 & -0.8797\\
\(t_d = 1000\) & -0.8341 & -0.8097 & -0.8076 & -0.7686\\
\hline
\end{tabular}
\end{center}
