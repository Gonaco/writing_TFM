
\chapter{Results}
\label{sec:orgee6d2af}

In this chapter the simulation results are presented with reference to the aim of the thesis, which was to study the mapping metrics.
Their precision when assessing the mapping quality and their relation with the error amount after mapping.
First, in the \hyperref[sec:org135a114]{Impact of mapping in the overall circuit error} section we show how the mapping procedure affects the general increment of error.
After that, we analyze the metrics and their correlation with the error increase in the \hyperref[sec:org89e3b29]{Analysis of the mapping metrics} section.
Finally, in the \hyperref[sec:org81252d3]{Regression analysis} section, after a regression study, we offer a way to predict the amount of errors a circuit could get depending on the its characteristics.

\section{Impact of mapping in the overall circuit error}
\label{sec:org135a114}
After the selection of benchmarks in the \href{chapter-4.org}{Benchmarks} section, we started their simulations with the simulation framework.
Not surprisingly, some of the benchmarks either had very long simulation times or were even impossible to simulate in our lab servers.
In most of those cases the circuits had more than ten qubits.
As we mentioned before throughout this thesis, the simulation of quantum systems is computationally exhausting.
As the number of qubits or the longitude of the circuit increase, the harder is to simulate them.
Indeed, it is a critical issue in our case, as soon as we need to run multiple simulations in a complex error model.
Therefore, as it can be seen in Tab. \ref{tab:map_selected_benchs}, we address that the final benchmark selection has a limitation in the number of qubits.

\begin{table}[htbp]
\caption{\label{tab:org38ab17c}
Table of the selected benchmarks to be mapped.}
\centering
\small
\begin{tabular}{lrrr}
\hline
Benchmark & \# qubits & \# gates & two-qubit gates (percentage)\\
\hline
4gt11\(_{\text{82}}\) & 5 & 27 & 0.667\\
4gt12\(_{\text{v1}}\)\(_{\text{89}}\) & 6 & 228 & 0.439\\
4gt4\(_{\text{v0}}\)\(_{\text{72}}\) & 6 & 258 & 0.438\\
4mod5\(_{\text{bdd}}\)\(_{\text{287}}\) & 7 & 70 & 0.443\\
4mod5\(_{\text{v0}}\)\(_{\text{20}}\) & 5 & 20 & 0.500\\
alu\(_{\text{bdd}}\)\(_{\text{288}}\) & 7 & 84 & 0.452\\
alu\(_{\text{v0}}\)\(_{\text{27}}\) & 5 & 36 & 0.472\\
decod24\(_{\text{bdd}}\)\(_{\text{294}}\) & 6 & 73 & 0.438\\
decod24\(_{\text{enable}}\)\(_{\text{126}}\) & 6 & 338 & 0.441\\
graycode6\(_{\text{47}}\) & 6 & 5 & 1.000\\
ham3\(_{\text{102}}\) & 3 & 20 & 0.550\\
hwb4\(_{\text{49}}\) & 5 & 233 & 0.459\\
mod10\(_{\text{176}}\) & 5 & 178 & 0.438\\
mod5adder\(_{\text{127}}\) & 6 & 555 & 0.431\\
mod5d1\(_{\text{63}}\) & 5 & 22 & 0.591\\
mod8\(_{\text{10}}\)\(_{\text{177}}\) & 6 & 440 & 0.445\\
one\(_{\text{two}}\)\(_{\text{three}}\)\(_{\text{v1}}\)\(_{\text{99}}\) & 5 & 132 & 0.447\\
one\(_{\text{two}}\)\(_{\text{three}}\)\(_{\text{v3}}\)\(_{\text{101}}\) & 5 & 70 & 0.457\\
rd32\(_{\text{v0}}\)\(_{\text{66}}\) & 4 & 34 & 0.471\\
sf\(_{\text{274}}\) & 6 & 781 & 0.430\\
sf\(_{\text{276}}\) & 6 & 778 & 0.432\\
sym6\(_{\text{145}}\) & 7 & 3888 & 0.438\\
xor5\(_{\text{254}}\) & 6 & 7 & 0.714\\
\hline
\end{tabular}
\end{table}


As explained in the \href{chapter-4.org}{Analysis framework} section, after running the benchmarks, the results obtained are the fidelity, the probability of success and the Quantum Volume.
We also extract other metrics like the number of SWAPs added and the depth of the circuits, between other circuit statistics.
To match the three router algorithms developed in our group (see \href{chapter-3.org}{Mapping model}), we compiled the list of benchmarks with four different configurations.
We defined one configuration per router and another one without mapping the algorithms to any device.
For a fair comparison, the non-mapped configuration decomposes the circuit gates in the SC-17 gate set (see Fig. \ref{fig:decompositions}).
We also tried different configurations regarding the decoherence time and the error rates in order to study the mappers in different error regimes.


The framework results immediately confirmed that the mapping procedure affects the errors amount in the quantum system.
In Fig. \ref{fig:orgdf91e16} we highlight the difference in fidelity of some benchmarks before and after being mapped.
The mapped data we plot comes from the router \texttt{minextendrc}.
We can see how the fidelity is smaller for long circuits -- like \texttt{sf\_274} or \texttt{mod5adder\_127} -- than for the short ones -- \texttt{graycode6\_47} or \texttt{xor5\_254}.
We show the exact result values in \href{appendix-1.org}{Appendix A}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/f_diff_bar_plot.eps}
\caption{\label{fig:orgdf91e16}
Difference of fidelities before and after mapping with the \texttt{minextendrc} router for five different benchmarks.}
\end{figure}

\section{Analysis of the mapping metrics}
\label{sec:org89e3b29}
Once we acknowledged the behaviour of the error growth due to the mapping process, it was the time to understand which circuit parameter affects it the most.
In this section we evaluate the behaviour and quality of the mapping metrics.
From the classical ones -- number of SWAPs and depth -- to the ones we proposed -- fidelity, probability of success and Quantum Volume.



First we analyze the probability of success and fidelity correlation.
As expected, our experiments prove that both metrics are highly correlated in a logarithmic fashion.
We also appreciated the fact that the probability of success is always higher than the fidelity.
This could suggest that the measurement is 'correcting' circuit errors colliding the state in the correct result, instead of the wrong one.
It is a 'good' mistake that results in the expected solution.
Nevertheless, this behaviour could be caused by the fact that our algorithms are deterministic.
Another knowledge we inherited from the results is that the closer the values go to 0 the more chaotic and random the values will get.
And, also, that no matter how error prone is the system, the probability of success and the fidelity will be correlated in a similar fashion.



In Fig. \ref{fig:orgc7ac6db} we plot the results of the framework in terms of probability of success and fidelity. 
For this figure and the figures from now on in this section, each dot is a different benchmark configuration and the colors represent different decoherence times, blue for 30 \(\mu s\) and orange for 10 \(\mu s\).
Fig. \ref{fig:orgc7ac6db} highlights a logarithmic correlation as well as the bias between both metrics, proving the fact that the probability of success is always higher than the fidelity.
For instance, for values around 0.6 in fidelity, the probability of success has a value bigger than 0.7 for the majority of the samples.
Note that for values close to 1, fidelity and probability of success tend to be equal and linear, which make sense; if some circuit has almost no error, both metrics will be equally good.
At the same time, the closer the values are to 0, the more spread the samples are, [proving the chaotic behaviour]
It can be seen that the inclination and bias of the fitting line changes depending on the decoherence time of the system.
Showing that, the higher amount of errors in the system, the more diagonal the fitting line gets.
The cluster that appears between the 0.9 and 1.0 values is due to the amount of simple benchmarks in the selection, as we previously mentioned.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/f_ps_correlation.eps}
\caption{\label{fig:orgc7ac6db}
Correlation between fidelity and probability of success for two different decoherence times}
\end{figure}




We use the Pearson correlation to measure which metric is the most correlated one with fidelity and probability of success.
As it can be seen in the Pearson values (Tab. [REFER TO TAB]) the metric most correlated is the number of two-qubit gates.
These results hold the fact that the quality of the mapping depends on the longitude of the targeted circuit, making crucial the mapping quality for small benchmarks.


[CORRELATION TABLE]

Quantum's Volume small lack of correlation can be attributed to the imprecise formula that we chose to calculate it.
Given that this was only a preliminary attempt to include the Quantum Volume as mapping metric it is hardly surprising that the results are not directly correlating it with the mapping quality.
The results of the main mapping metrics against fidelity are depicted in Fig. \ref{fig:f_metrics_correlation}.
As in previous plots, we show the results for two different decoherence times, 30 \(\mu s\) (blue dots) and 10 \(\mu s\) (orange dots).
We observe that, for all the cases, the fidelity decreases with an inverse exponential behaviour.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/f_metrics_correlation_poly.eps}
\caption{\label{fig:org7ebacc1}
Correlation between fidelity and the mapping metrics.}
\end{figure}

The correlation between the probability of success and the other metrics can be see in Fig. \ref{fig:ps_metrics_correlation}.
As with the study on fidelity correlation we plot two decoherence times, 30 \(\mu s\) (blue) and 10 \(\mu s\) (orange).
We also observe a decreasing behaviour, although the shape is not as clear as in the case of fidelity.
This could be provoked by the final error added by the measurement gate and by the fact that, most of the times, the measurement is correcting the wrong solutions.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/ps_metrics_correlation.eps}
\caption{\label{fig:org2a9dedc}
Correlation between probability of success and the mapping metrics.}
\end{figure}

We can also see in both figures that, as announced before, there is a cluster of benchmarks with high fidelity and high probability of success.
This happens because of the high concentration of small benchmarks, due to the simulation difficulties.
On the contrary, the rest of the values are a bit sparsed.

\subsection{Regression analysis}
\label{sec:org81252d3}
After analyzing the correlation between metrics we begin the regression analysis.
We aim to understand the behaviour of our data and be able to predict the fidelity or the probability of success given the circuit parameters and the error rates.

Our regression model is based on a double regression model.
First our model learns and predicts from the samples with Support Vector Machines (SVM) using as kernel the Radial Basis Function (RBF) and then, with those predictions we feed a second regression model in order to shaped the first analysis.
The SVM model was chosen because it is one of the most practical, rapid and ergonomic ways to commit a regression model.
The flexibility of the regression model is critical in our case due to the non-linear aspect of the data.
For the second regression model we use different, much simpler, regression algorithms with the intention to find the model that fits the data the best.
In the Fig. \ref{fig:f_metrics_correlation} and \ref{fig:ps_metrics_correlation} we present fitting lines predicted after the regression model we used.

\begin{enumerate}
\item Fidelity
\label{sec:org201cb0a}

For the model between the metrics and the fidelity we opted for a [POLYNOMIAL/EXPONENTIAL] regression model as the second regression algorithm.

\item Probability of success
\label{sec:orgae27aaa}

In the case of the probability of success model we opted for a [LINEAL] regression mode for the second regression process.
\end{enumerate}

\section{Advice}
\label{sec:orge6b6e64}
