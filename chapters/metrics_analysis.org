*** Intro                                                          :ignore:

Once we acknowledge the behaviour of the fidelity decrease due to the mapping process, in this section, we will analyze which circuit parameter affects the amount of error the most.
We consider that the best mapping is the one adding the less possible errors to the original circuit.
If we are able to answer which metric is more correlated with the appearance of errors in the circuit, we will know the most critical metric for the mapping quality.
Therefore, in this section we evaluate the behaviour of the number of gates, the number of two-qubit gates -- in accordance with the number of SWAPs --, the depth and the Quantum Volume metrics in comparison with the error growth, or what is the same, the fidelity and the probability of success.

*** Fidelity and probability of success correlation                :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT

First we analyze the probability of success and fidelity correlation.
As we explained in the \hyperref[sec:org0c7b2c2]{Fidelity and Probability of success} section, one of the main differences between these two metrics is that the probability of success takes into account the measurement process of the qubits, while fidelity does not.
This makes the probability of success not only sensitive to the errors related with the measurement but also to the inherent non-deterministic behaviour of it.
This means that, although our benchmarks are deterministic, a circuit could result erroneously in a state in superposition due to the quantum noise, forcing the measurement to collapse to either the correct or the wrong state.
For example, let us say that we are expecting ~1~ as the result of a given qubit but, before measuring instead of $|1\rangle$ we got $\sqrt{0.3} |0\rangle + \sqrt{0.7} |1\rangle$.
In this case, without taking into account the measurement errors, the measurement will result in ~0~ with a 30% chance and ~1~ with a 70% chance.
For this reason, the probability of success tends to be more chaotic in regimes with high amount of errors; as we will throughout this section.


**** What we show in the figure                                   :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT


In Fig. [[fig:f_ps_correlation_with_meas_error]] we plot the correlation of probability of success and fidelity. 
For this figure and the figures from now on in this section, each dot is a different benchmark configuration (see [[id:15254cfb-b82c-47a3-b8e8-8eb08de47f54][Appendix A]]) and the colors represent different decoherence times, blue for 30 $\mu s$ and orange for 10 $\mu s$.


#+caption: Correlation between fidelity and probability of success for two different decoherence times
#+NAME: fig:f_ps_correlation_with_meas_error
#+ATTR_LATEX: :width 0.7\textwidth
[[file:figures/f_ps_correlation.eps]]

# In Fig. [[fig:f_ps_correlation_no_meas_error]] we plot the results of the framework with and without introducing errors in the measurement.
# The blue dots are, as in Fig. [[fig:f_ps_correlation_with_meas_error]], the different benchmark configurations simulated with a decoherence time of 30 $\mu s$ and measurement errors.
# On the other hand, this time, the orange dots represent benchmark configurations simulated with a decoherence time of 30 $\mu s$ without measurement errors.
# As we expected, the ... [We are not showing anything in this figure!]

# #+caption: Correlation between fidelity and probability of success for the case of having errors in the measurement and not having errors
# #+NAME: fig:f_ps_correlation_no_meas_error
# #+ATTR_LATEX: :width 0.6\textwidth
# [[file:figures/f_ps_correlation_no_meas_error.png]]


***** With measurement error                                   :noexport:

SIGO FILTRANDO FIDELITY > 0.5

#+BEGIN_SRC c

Analysis For Decoherence Time = 3000 and Error Measurement = 0.005

        -------------------------------

        -- Correlation between the Fidelity and Probability of Success

Polynomial function:

0.5914 x + 0.4081
----------------------------

(0.9192199104316764, 3.767553069709704e-25)

        Analysis For Decoherence Time = 1000 and Error Measurement = 0.005

        -------------------------------

        -- Correlation between the Fidelity and Probability of Success

Polynomial function:

0.7122 x + 0.3026
----------------------------

(0.9560273488297862, 4.0669039495216075e-12)

#+END_SRC


***** No Measurement error                                     :noexport:

SIGO FILTRANDO FIDELITY > 0.5


#+BEGIN_SRC C

Analysis For Decoherence Time = 3000 and Error Measurement = 0.005

        -------------------------------

        -- Correlation between the Fidelity and Probability of Success

Polynomial function:

0.5914 x + 0.4081
----------------------------

(0.9192199104316764, 3.767553069709704e-25)

        Analysis For Decoherence Time = 3000 and Error Measurement = 0

        -------------------------------

        -- Correlation between the Fidelity and Probability of Success

Polynomial function:

0.6267 x + 0.3777
----------------------------

(0.9358217171375378, 1.410870124624645e-26)

#+END_SRC

**** What we see in the results                                   :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT

# What I can see from the figure ref:fig:f_ps_correlation_with_meas_error:

# - Probability of success and fidelity are correlated
# - It can be seen that most of the samples are above the $x=y$ line. What means that, in general, probability of success is higher than the fidelity. This suggest me that the measurement is making 'good' mistakes. Nevertheless, this behaviour could be happening because of the low measurement error rates from SC-17 or due to the fact that our algorithms are deterministic. It is possible that, results in superposition would dilute this behaviour.
# - We can see how the closer the samples get to 0 fidelity or 0 probability of success, the more chaotic the values are. Actually we can delimit two parts in the graph: above and below 0.6 in fidelity -- around 0.7 in probability of success. Above 0.6 we observe an almost linear behaviour and below 0.6 we start seeing more chaotic results. This behaviour happens because, whenever a circuit accumulates a high amount of errors, fidelity and probability of success have low but almost random values. On the other hand, if some circuit has almost no error, both metrics will be equally good; explaining the values close to 1.
# - Finally, we see that most of the orange samples are below the 0.6 fidelity value; or what is the same, most of the benchmarks simulated with the low decoherence time have more chaotic results. This can be explained by the fact that the correctness of circuits that are run in lower error rate regimes will have much worse results in systems with higher error rates.

As expected, our experiments prove that both metrics are highly correlated.
We also appreciated the fact that most of the samples are above the $x=y$ line; meaning that the probability of success, in general, is higher than the fidelity.
This could suggest that the measurement is 'correcting' circuit errors colliding the state in the correct result, instead of the wrong one.
It seems that the measurement is committing 'good' mistakes that result in the expected solution.
Nevertheless, this behaviour could be happening because of the indeterministic behaviour of the measurement or due to the fact that our algorithms are deterministic.
It would possible that this 'correcting- errors behaviour' is diluted while running quantum algorithms that result in a quantum state in superposition.
Further research should be done in order to understand this behaviour.

Another observation we see is that, the closer the samples get to 0 fidelity or 0 probability of success, the more chaotic the values are. Actually we can delimit two parts in the graph: above and below 0.7 in fidelity -- around 0.8 in probability of success. Above 0.7 we observe an almost linear behaviour and below 0.6 we start seeing more chaotic results. This behaviour happens because, whenever a circuit accumulates a high amount of errors, fidelity or probability of success low but almost random values.
Finally, we see that most of the orange samples are below the 0.6 fidelity value; or what is the same, most of the benchmarks simulated with the low decoherence time have more chaotic values. 
This can be explained by the fact that the correctness of circuits that are run in lower error rate regimes will have much better results in systems with higher error rates.

*** Metrics behaviour                                              :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT

Regarding the rest of the metrics, below we analyze their relation with the fidelity and probability of success.
Apart from the metrics we described throughout this thesis, we did the analysis with the total number of gates as well, because we noticed a high correlation between that and the fidelity or the probability of success, although more with the first one.

**** What we see in the figures                                   :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT

The results of the main mapping metrics against fidelity are depicted in Fig. [[ref:fig:f_metrics_correlation][ref:fig:f_metrics_correlation]].
We observe that, for all the cases, the fidelity decreases with a behaviour that looks inverse exponential and that decreases faster for small decoherence times.
Certainly, the shorter the decoherence times we use the more benchmarks will have non-useful results.
We can also see how fidelity never goes to zero, but it gets constant around 0.2, giving random results.
We consider the point where fidelity is constant as the limit in terms of each one of the variables.
We plot a line for the orange samples to mark this point.
Finally, it can be seen how the number of gates is the metric most related with the fidelity; it is the one with the samples more ordered.

#+caption: Correlation between fidelity and the mapping metrics.
#+NAME: fig:f_metrics_correlation
#+ATTR_LATEX: :width \textwidth
[[file:figures/f_metrics_correlation_poly.eps]]

The correlation between the probability of success and the other metrics can be see in Fig. [[ref:fig:ps_metrics_correlation][ref:fig:ps_metrics_correlation]].
We also observe a certain correlation between the metrics and the probability of success.
We observe a decreasing behaviour, although the samples are much more spread in this case than in the case of the fidelity. We believe that this is provoked by the indeterministic effect in the nature of the measurement.
We can see again that for shorter decoherence times the probability of success is lower and descends much faster. 


# We also observe a decreasing behaviour, although the shape is not as clear as in the case of fidelity.
# This could be provoked, again, by the final error added by the measurement gate and by the fact that, most of the times, the measurement is correcting the wrong solutions.
# The figure also highlights how the fast probability of success decreases depending on the decoherence time.

#+caption: Correlation between probability of success and the mapping metrics.
#+NAME: fig:ps_metrics_correlation
#+ATTR_LATEX: :width \textwidth
[[file:figures/ps_metrics_correlation.eps]]

# We can also see in both figures that, as announced before, there is a cluster of benchmarks with high fidelity and high probability of success.
# This happens because of the high concentration of small benchmarks, due to the simulation difficulties.
# On the contrary, the rest of the values are a bit spread.

**** How we analyze it                                            :ignore:

In general, we observe a high correlation between all of the metrics and the fidelity and the probability of success; so in order to differ between the metrics, we calculate the Pearson correlation coefficients to measure the correlation quality between them for different amounts of errors in a circuit.
Note that the Pearson coefficient measures linear correlations and, as it looks like in Fig. [[ref:fig:f_metrics_correlation][ref:fig:f_metrics_correlation]], the metrics behave in an inverse exponential fashion against fidelity.
For this reason, in order to see the real correlation between the metrics and the fidelity, we applied a $log(\cdot)$ transformation to our fidelity data in order to make it linear for the Pearson calculation.

**** What we see in the results                                   :ignore:

#+BEGIN_EXPORT latex

#+END_EXPORT

As it can be seen in the Pearson values (Tab. ref:tab:pearson_corr_f and Tab. ref:tab:pearson_corr_ps) the most correlated metric is the number of two-qubit gates.
These results hold the fact that the quality of the mapping depends directly on the length of the targeted circuit before it is being mapped.
For example, a long circuit well mapped will have always worse results in fidelity or probability of success than a short circuit badly mapped.
As Tab. ref:tab:pearson_corr_f and Tab. ref:tab:pearson_corr_ps highlight, we have a worse correlation for the shorter decoherence time.
This lack of correlation can be attributed to the fact that the majority of the samples with $t_d = 1000$ are highly affected by the errors and, therefore, the samples have more random values.

Moreover, contrary to our expectations, the Quantum Volume was the least correlated metric.
This small lack of correlation can be attributed to the imprecise formula that we chose to compute it.
Future work needs to be done to devise a better formula.

Finally, if we compare both tables, we can see that the fidelity is more correlated with the metrics than the probability of success.
It is very likely that the reason for this result is that the measurement error together with the measurement indeterministic behaviour  adds more noise and spreads our samples.
Also the 'correcting-errors behaviour' of the measurement should be taken into account.

#+caption: Pearson correlation coefficient of the log transformation of fidelity against the metrics($\rho _{log(f),Y}$), where $Y$ is one of the four metrics we analyze
#+NAME: tab:pearson_corr_f
#+ATTR_LATEX: :booktabs :environment :font :width \textwidth :float 
|--------------+------------+----------------------+---------+---------|
|              | # of Gates | # of Two-qubit gates |   Depth |   $V_Q$ |
|--------------+------------+----------------------+---------+---------|
| $t_d = 3000$ |    -0.9730 |              -0.9600 | -0.9455 | -0.9118 |
| $t_d = 1000$ |    -0.8466 |              -0.8135 | -0.8093 | -0.7736 |
|--------------+------------+----------------------+---------+---------|
#+TBLFM: 

#+caption: Pearson correlation coefficient for the probability of success against the metrics ($\rho _{p_s,Y}$), where $Y$ is one of the four metrics we analyze
#+NAME: tab:pearson_corr_ps
#+ATTR_LATEX: :booktabs :environment :font :width \textwidth :float
|--------------+------------+----------------------+---------+---------|
|              | # of Gates | # of Two-qubit gates |   Depth |   $V_Q$ |
|--------------+------------+----------------------+---------+---------|
| $t_d = 3000$ |    -0.9363 |              -0.9248 | -0.9179 | -0.8797 |
| $t_d = 1000$ |    -0.8341 |              -0.8097 | -0.8076 | -0.7686 |
|--------------+------------+----------------------+---------+---------|
#+TBLFM: 


*** BIB                                                   :ignore:noexport:

bibliography:../thesis_plan.bib
bibliographystyle:plain
